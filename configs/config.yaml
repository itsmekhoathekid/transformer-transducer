training:
  epochs: 100
  batch_size: 2
  save_path: "/home/anhkhoa/transformer_transducer_speeQ/save_folder"
  train_path : "/home/anhkhoa/transformer_transducer_speeQ/data/train.json"
  dev_path : "/home/anhkhoa/transformer_transducer_speeQ/data/dev.json"
  test_path : "/home/anhkhoa/transformer_transducer_speeQ/data/dev.json"
  vocab_path : "/home/anhkhoa/transformer_transducer_speeQ/data/vocab.json"
  reload: False
  log_file: "/home/anhkhoa/transformer-transducer/transformer_transducer_log.txt"
  


optim:
  type: adam
  lr: 0.0005
  weight_decay: 0.0001
  decay_rate: 0.5


model:
   in_features: 40
   n_enc_layers: 2
   n_dec_layers: 2
   d_model: 64
   ff_size: 128
   h: 4
   joint_size: 128
   p_dropout: 0.1
   model_name: "transformer_transducer"


rnnt_loss:
  blank: 4
  reduction: "mean" 

scheduler:
  # type: "LROPT" # Options: [noam, LROPT]
  lr_initial: 0.0005
  n_warmup_steps: 25000

