training:
  epochs: 100
  batch_size: 2
  save_path: "workspace/transformer-transducer/saves"
  train_path : "workspace/dataset/train_w2i.json"
  dev_path : "workspace/dataset/test_w2i.json"
  test_path : "workspace/dataset/test_w2i.json"
  vocab_path : "workspace/dataset/vocab_w2i.json"
  reload: False
  log_file: "workspace/transformer-transducer/transformer_transducer_log.txt"

fbank:
  n_mels: 80
  n_fft: 512
  win_length: 25
  hop_length: 10
  sample_rate: 16000


optim:
  type: sgd 
  lr: 0.0003
  weight_decay: 0.0001
  momentum: 0.9
  decay_rate: 0.5
  nesterov: None


model:
   in_features: 80
   in_features: 320
   n_enc_layers: 3
   n_dec_layers: 2
   d_model: 512
   ff_size: 2048
   h: 8
   joint_size: 1024
   p_dropout: 0.1
   model_name: "transformer_transducer"


rnnt_loss:
  blank: 0
  reduction: "mean" 

scheduler:
  # type: "LROPT" # Options: [noam, LROPT]
  lr_initial: 0.0003
  n_warmup_steps: 25000
